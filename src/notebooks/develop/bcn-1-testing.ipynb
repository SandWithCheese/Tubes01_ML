{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthcheck\n"
     ]
    }
   ],
   "source": [
    "from ffnn.healthcheck import healthcheck\n",
    "healthcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffnn.ffnn import FFNN\n",
    "from ffnn.types import ActivationFunction, LossFunction, WeightInitializer, WeightsSetup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer initialized: 2 -> 2\n",
      "Activation: ActivationFunction.SIGMOID\n",
      "Weights setup: WeightsSetup(initializer=<WeightInitializer.UNIFORM: 2>, lower_bound=0, upper_bound=1, mean=None, variance=None, seed=42)\n",
      "Weight initializer: WeightInitializer.UNIFORM\n",
      "\n",
      "Layer initialized: 2 -> 2\n",
      "Activation: ActivationFunction.SOFTMAX\n",
      "Weights setup: WeightsSetup(initializer=<WeightInitializer.NORMAL: 3>, lower_bound=None, upper_bound=None, mean=0, variance=1, seed=42)\n",
      "Weight initializer: WeightInitializer.NORMAL\n",
      "\n",
      "FFNN initialized\n",
      "Layer sizes: [2, 2, 2]\n",
      "Activation functions: [<ActivationFunction.SIGMOID: 3>, <ActivationFunction.SOFTMAX: 5>]\n",
      "Loss function: LossFunction.MEAN_SQUARED_ERROR\n",
      "Weights setup: [WeightsSetup(initializer=<WeightInitializer.UNIFORM: 2>, lower_bound=0, upper_bound=1, mean=None, variance=None, seed=42), WeightsSetup(initializer=<WeightInitializer.NORMAL: 3>, lower_bound=None, upper_bound=None, mean=0, variance=1, seed=42)]\n",
      "Learning rate: 0.5\n",
      "Batch size: 1\n",
      "Epochs: 2\n",
      "Verbose: True\n",
      "Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# 2 node di input layer, 2 node di hidden layer, 2 node di output layer\n",
    "layer_sizes = [2, 2, 2]\n",
    "\n",
    "activation_functions = [\n",
    "    ActivationFunction.SIGMOID, \n",
    "    ActivationFunction.SOFTMAX\n",
    "]\n",
    "\n",
    "loss_function = LossFunction.MEAN_SQUARED_ERROR\n",
    "\n",
    "weights_setup = [\n",
    "    WeightsSetup(initializer=WeightInitializer.UNIFORM, lower_bound=0, upper_bound=1, seed=1),\n",
    "    WeightsSetup(initializer=WeightInitializer.NORMAL, mean=0, variance=1, seed=1),\n",
    "]\n",
    "\n",
    "model = FFNN(\n",
    "    layer_sizes=layer_sizes, \n",
    "    activation_functions=activation_functions, \n",
    "    loss_function=loss_function, \n",
    "    weights_setup=weights_setup,\n",
    "    epochs=2,\n",
    "    learning_rate=0.5,\n",
    "    verbose=True,\n",
    "    random_state=42,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([\n",
    "    [0.05, 0.1],\n",
    "])\n",
    "\n",
    "target_data = np.array([\n",
    "    [0.01, 0.99],\n",
    "])\n",
    "\n",
    "model.set_weights([\n",
    "    np.array([\n",
    "        [0.15, 0.25],\n",
    "        [0.20, 0.30],\n",
    "    ]),\n",
    "    np.array([\n",
    "        [0.40, 0.50],\n",
    "        [0.45, 0.55],\n",
    "    ]),\n",
    "])\n",
    "\n",
    "model.set_biases([\n",
    "    np.array([0.35, 0.35]),\n",
    "    np.array([0.60, 0.60]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 0.2118587943114401\n",
      "Epoch 2, Batch 0, Loss: 0.13318136204220613\n"
     ]
    }
   ],
   "source": [
    "model.fit(input_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "y = np.eye(10)[y.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 784) (49000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer initialized: 784 -> 256\n",
      "Activation: ActivationFunction.RELU\n",
      "Weights setup: WeightsSetup(initializer=<WeightInitializer.XAVIER: 4>, lower_bound=None, upper_bound=None, mean=None, variance=None, seed=42)\n",
      "Weight initializer: WeightInitializer.XAVIER\n",
      "\n",
      "Layer initialized: 256 -> 128\n",
      "Activation: ActivationFunction.RELU\n",
      "Weights setup: WeightsSetup(initializer=<WeightInitializer.XAVIER: 4>, lower_bound=None, upper_bound=None, mean=None, variance=None, seed=42)\n",
      "Weight initializer: WeightInitializer.XAVIER\n",
      "\n",
      "Layer initialized: 128 -> 10\n",
      "Activation: ActivationFunction.SOFTMAX\n",
      "Weights setup: WeightsSetup(initializer=<WeightInitializer.XAVIER: 4>, lower_bound=None, upper_bound=None, mean=None, variance=None, seed=42)\n",
      "Weight initializer: WeightInitializer.XAVIER\n",
      "\n",
      "FFNN initialized\n",
      "Layer sizes: [784, 256, 128, 10]\n",
      "Activation functions: [<ActivationFunction.RELU: 2>, <ActivationFunction.RELU: 2>, <ActivationFunction.SOFTMAX: 5>]\n",
      "Loss function: LossFunction.CATEGORICAL_CROSS_ENTROPY\n",
      "Weights setup: [WeightsSetup(initializer=<WeightInitializer.XAVIER: 4>, lower_bound=None, upper_bound=None, mean=None, variance=None, seed=42), WeightsSetup(initializer=<WeightInitializer.XAVIER: 4>, lower_bound=None, upper_bound=None, mean=None, variance=None, seed=42), WeightsSetup(initializer=<WeightInitializer.XAVIER: 4>, lower_bound=None, upper_bound=None, mean=None, variance=None, seed=42)]\n",
      "Learning rate: 0.001\n",
      "Batch size: 256\n",
      "Epochs: 100\n",
      "Verbose: True\n",
      "Random state: 42\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [784, 256, 128, 10]\n",
    "# layer_sizes = [784, 256, 10]\n",
    "\n",
    "activation_functions = [\n",
    "    ActivationFunction.RELU,\n",
    "    ActivationFunction.RELU,\n",
    "    ActivationFunction.SOFTMAX,\n",
    "]\n",
    "\n",
    "loss_function = LossFunction.CATEGORICAL_CROSS_ENTROPY\n",
    "\n",
    "weights_setup = [\n",
    "    WeightsSetup(WeightInitializer.XAVIER, seed=42),\n",
    "    WeightsSetup(WeightInitializer.XAVIER, seed=42),\n",
    "    WeightsSetup(WeightInitializer.XAVIER, seed=42),\n",
    "    # WeightsSetup(WeightInitializer.NORMAL, mean=0, variance=1, seed=42),\n",
    "    # WeightsSetup(WeightInitializer.NORMAL, mean=0, variance=1, seed=42),\n",
    "    # WeightsSetup(WeightInitializer.NORMAL, mean=0, variance=1, seed=42),\n",
    "    # WeightsSetup(WeightInitializer.UNIFORM, lower_bound=0, upper_bound=1, seed=42),\n",
    "    # WeightsSetup(WeightInitializer.UNIFORM, lower_bound=0, upper_bound=1, seed=42),\n",
    "    # WeightsSetup(WeightInitializer.UNIFORM, lower_bound=0, upper_bound=1, seed=42),\n",
    "]\n",
    "\n",
    "model = FFNN(\n",
    "    layer_sizes=layer_sizes, \n",
    "    activation_functions=activation_functions, \n",
    "    loss_function=loss_function, \n",
    "    weights_setup=weights_setup,\n",
    "    epochs=100,\n",
    "    verbose=True,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=256,\n",
    "    random_state=42,\n",
    "    l1_lambda=0.0001,  # L1 regularization\n",
    "    l2_lambda=0.0001,  # L2 regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 31.471876384161092\n",
      "Epoch 1, Batch 10, Loss: 8.784296869657485\n",
      "Epoch 1, Batch 20, Loss: 6.5092403271570305\n",
      "Epoch 1, Batch 30, Loss: 4.220117398759601\n",
      "Epoch 1, Batch 40, Loss: 3.4357304617404036\n",
      "Epoch 1, Batch 50, Loss: 2.8790575763387967\n",
      "Epoch 1, Batch 60, Loss: 2.474030237839226\n",
      "Epoch 1, Batch 70, Loss: 4.093222627721109\n",
      "Epoch 1, Batch 80, Loss: 3.427275810732173\n",
      "Epoch 2, Batch 0, Loss: 1.5303402952602863\n",
      "Epoch 2, Batch 10, Loss: 1.9618524149771503\n",
      "Epoch 2, Batch 20, Loss: 1.4023701423255595\n",
      "Epoch 2, Batch 30, Loss: 1.690921068716312\n",
      "Epoch 2, Batch 40, Loss: 1.7911214071820811\n",
      "Epoch 2, Batch 50, Loss: 1.7747943300708888\n",
      "Epoch 2, Batch 60, Loss: 0.8810143755507551\n",
      "Epoch 2, Batch 70, Loss: 1.0741181611351978\n",
      "Epoch 2, Batch 80, Loss: 1.1810968907529817\n",
      "Epoch 3, Batch 0, Loss: 1.0839652361625554\n",
      "Epoch 3, Batch 10, Loss: 1.5366273183451489\n",
      "Epoch 3, Batch 20, Loss: 1.0143342029184028\n",
      "Epoch 3, Batch 30, Loss: 0.8996541227389298\n",
      "Epoch 3, Batch 40, Loss: 1.455488393688074\n",
      "Epoch 3, Batch 50, Loss: 1.1127622786083966\n",
      "Epoch 3, Batch 60, Loss: 0.4596963503785502\n",
      "Epoch 3, Batch 70, Loss: 1.1262604242799341\n",
      "Epoch 3, Batch 80, Loss: 0.8550900837984976\n",
      "Epoch 4, Batch 0, Loss: 0.7518609996197814\n",
      "Epoch 4, Batch 10, Loss: 0.558297199504194\n",
      "Epoch 4, Batch 20, Loss: 0.5917418063141671\n",
      "Epoch 4, Batch 30, Loss: 0.29562282390869254\n",
      "Epoch 4, Batch 40, Loss: 1.1968683104257345\n",
      "Epoch 4, Batch 50, Loss: 0.5848321128660234\n",
      "Epoch 4, Batch 60, Loss: 1.3689104633678006\n",
      "Epoch 4, Batch 70, Loss: 0.6554984703798729\n",
      "Epoch 4, Batch 80, Loss: 0.5420808557809312\n",
      "Epoch 5, Batch 0, Loss: 0.6900781432219072\n",
      "Epoch 5, Batch 10, Loss: 0.8316965639551589\n",
      "Epoch 5, Batch 20, Loss: 0.7121713393865914\n",
      "Epoch 5, Batch 30, Loss: 0.40634114378454894\n",
      "Epoch 5, Batch 40, Loss: 0.798873843772609\n",
      "Epoch 5, Batch 50, Loss: 0.5022103350196958\n",
      "Epoch 5, Batch 60, Loss: 0.557950187943832\n",
      "Epoch 5, Batch 70, Loss: 0.6990759115483408\n",
      "Epoch 5, Batch 80, Loss: 0.9069403963548937\n",
      "Epoch 6, Batch 0, Loss: 0.6519853113872573\n",
      "Epoch 6, Batch 10, Loss: 0.3871535973991482\n",
      "Epoch 6, Batch 20, Loss: 0.8670589280350913\n",
      "Epoch 6, Batch 30, Loss: 0.2226689872580123\n",
      "Epoch 6, Batch 40, Loss: 0.842519964651654\n",
      "Epoch 6, Batch 50, Loss: 0.40700438631296754\n",
      "Epoch 6, Batch 60, Loss: 0.2841931784515065\n",
      "Epoch 6, Batch 70, Loss: 0.7923619380177032\n",
      "Epoch 6, Batch 80, Loss: 0.5650879993016727\n",
      "Epoch 7, Batch 0, Loss: 0.07563730474702521\n",
      "Epoch 7, Batch 10, Loss: 0.32991754907986814\n",
      "Epoch 7, Batch 20, Loss: 0.4386980058875093\n",
      "Epoch 7, Batch 30, Loss: 0.653582744920894\n",
      "Epoch 7, Batch 40, Loss: 0.3997510663082907\n",
      "Epoch 7, Batch 50, Loss: 0.16657149252738157\n",
      "Epoch 7, Batch 60, Loss: 0.6309178542316298\n",
      "Epoch 7, Batch 70, Loss: 0.7892484693532268\n",
      "Epoch 7, Batch 80, Loss: 0.6333083517819402\n",
      "Epoch 8, Batch 0, Loss: 0.3326692914886062\n",
      "Epoch 8, Batch 10, Loss: 0.7090843965277365\n",
      "Epoch 8, Batch 20, Loss: 0.429613416896955\n",
      "Epoch 8, Batch 30, Loss: 0.0047460721496001244\n",
      "Epoch 8, Batch 40, Loss: 0.5838233672916424\n",
      "Epoch 8, Batch 50, Loss: 0.3199527597569389\n",
      "Epoch 8, Batch 60, Loss: 0.7216720566104627\n",
      "Epoch 8, Batch 70, Loss: 0.4415052324736323\n",
      "Epoch 8, Batch 80, Loss: 0.13290631729200328\n",
      "Epoch 9, Batch 0, Loss: 0.3732565571895371\n",
      "Epoch 9, Batch 10, Loss: 0.4306389428438738\n",
      "Epoch 9, Batch 20, Loss: 0.5641040895700092\n",
      "Epoch 9, Batch 30, Loss: 0.18252436997577387\n",
      "Epoch 9, Batch 40, Loss: 0.1954999136071943\n",
      "Epoch 9, Batch 50, Loss: 0.7677144717760017\n",
      "Epoch 9, Batch 60, Loss: 0.1495217003701414\n",
      "Epoch 9, Batch 70, Loss: 0.14611997536307222\n",
      "Epoch 9, Batch 80, Loss: 0.490829838673514\n",
      "Epoch 10, Batch 0, Loss: 0.08162884628846419\n",
      "Epoch 10, Batch 10, Loss: 0.11002842477520997\n",
      "Epoch 10, Batch 20, Loss: 0.41455438712590675\n",
      "Epoch 10, Batch 30, Loss: 0.27756845817418746\n",
      "Epoch 10, Batch 40, Loss: 0.12390007451886398\n",
      "Epoch 10, Batch 50, Loss: 0.3069644992778204\n",
      "Epoch 10, Batch 60, Loss: 0.20472766962423086\n",
      "Epoch 10, Batch 70, Loss: 0.6963102377657255\n",
      "Epoch 10, Batch 80, Loss: 0.31576423880357385\n",
      "Epoch 11, Batch 0, Loss: 0.10372712173204868\n",
      "Epoch 11, Batch 10, Loss: 0.13119623679661568\n",
      "Epoch 11, Batch 20, Loss: 0.3443064899244295\n",
      "Epoch 11, Batch 30, Loss: 0.13855267754687922\n",
      "Epoch 11, Batch 40, Loss: 0.3357195368691738\n",
      "Epoch 11, Batch 50, Loss: 0.11986857543322232\n",
      "Epoch 11, Batch 60, Loss: 0.14441583591448742\n",
      "Epoch 11, Batch 70, Loss: 0.6693986279615969\n",
      "Epoch 11, Batch 80, Loss: 0.21523339873777192\n",
      "Epoch 12, Batch 0, Loss: 0.047774692688590865\n",
      "Epoch 12, Batch 10, Loss: 0.06883037921496503\n",
      "Epoch 12, Batch 20, Loss: 0.2813988033422543\n",
      "Epoch 12, Batch 30, Loss: 0.3472588734012423\n",
      "Epoch 12, Batch 40, Loss: 0.2932310486713332\n",
      "Epoch 12, Batch 50, Loss: 0.369373449182971\n",
      "Epoch 12, Batch 60, Loss: 0.2669439388839131\n",
      "Epoch 12, Batch 70, Loss: 0.0588337903606609\n",
      "Epoch 12, Batch 80, Loss: 0.5494120890435026\n",
      "Epoch 13, Batch 0, Loss: 0.2594678162838567\n",
      "Epoch 13, Batch 10, Loss: 0.2716105236034967\n",
      "Epoch 13, Batch 20, Loss: 0.24165093009424382\n",
      "Epoch 13, Batch 30, Loss: 0.3664530164714063\n",
      "Epoch 13, Batch 40, Loss: 0.08349340262367358\n",
      "Epoch 13, Batch 50, Loss: 0.0730529523536591\n",
      "Epoch 13, Batch 60, Loss: 0.22825322615201224\n",
      "Epoch 13, Batch 70, Loss: 0.33713800743637035\n",
      "Epoch 13, Batch 80, Loss: 0.344262496011779\n",
      "Epoch 14, Batch 0, Loss: 0.26414028069020024\n",
      "Epoch 14, Batch 10, Loss: 0.030698562208210645\n",
      "Epoch 14, Batch 20, Loss: 0.25117466011627654\n",
      "Epoch 14, Batch 30, Loss: 0.40220849648337903\n",
      "Epoch 14, Batch 40, Loss: 0.08417163629498906\n",
      "Epoch 14, Batch 50, Loss: 0.13021754584518902\n",
      "Epoch 14, Batch 60, Loss: 0.4943431736695168\n",
      "Epoch 14, Batch 70, Loss: 0.07931605387555657\n",
      "Epoch 14, Batch 80, Loss: 0.2131684501816455\n",
      "Epoch 15, Batch 0, Loss: 0.233405974206345\n",
      "Epoch 15, Batch 10, Loss: 0.10804158206967845\n",
      "Epoch 15, Batch 20, Loss: 0.0009004109946514055\n",
      "Epoch 15, Batch 30, Loss: 0.17061327922350483\n",
      "Epoch 15, Batch 40, Loss: 0.15930721006264761\n",
      "Epoch 15, Batch 50, Loss: 0.11240345653579292\n",
      "Epoch 15, Batch 60, Loss: 0.2636074794108442\n",
      "Epoch 15, Batch 70, Loss: 0.24259673221239814\n",
      "Epoch 15, Batch 80, Loss: 0.2566473374147228\n",
      "Epoch 16, Batch 0, Loss: 0.25819102506301417\n",
      "Epoch 16, Batch 10, Loss: 0.061968458215324464\n",
      "Epoch 16, Batch 20, Loss: 0.2456274445577598\n",
      "Epoch 16, Batch 30, Loss: 0.4111427416121122\n",
      "Epoch 16, Batch 40, Loss: 0.051113413854887636\n",
      "Epoch 16, Batch 50, Loss: 0.1565435957376334\n",
      "Epoch 16, Batch 60, Loss: 0.5120979134006889\n",
      "Epoch 16, Batch 70, Loss: 0.12324608225045111\n",
      "Epoch 16, Batch 80, Loss: 0.22183039600780485\n",
      "Epoch 17, Batch 0, Loss: 0.00426593544405567\n",
      "Epoch 17, Batch 10, Loss: 0.1544744149673186\n",
      "Epoch 17, Batch 20, Loss: 0.1128067980451875\n",
      "Epoch 17, Batch 30, Loss: 0.3446292948691125\n",
      "Epoch 17, Batch 40, Loss: 0.021172353434157222\n",
      "Epoch 17, Batch 50, Loss: 0.2405712022700225\n",
      "Epoch 17, Batch 60, Loss: 0.06513371093525266\n",
      "Epoch 17, Batch 70, Loss: 0.4117345904922882\n",
      "Epoch 17, Batch 80, Loss: 0.08654297553090129\n",
      "Epoch 18, Batch 0, Loss: 0.17555833185174718\n",
      "Epoch 18, Batch 10, Loss: 0.13491828703774356\n",
      "Epoch 18, Batch 20, Loss: 0.18502757689817212\n",
      "Epoch 18, Batch 30, Loss: 0.055453755301826536\n",
      "Epoch 18, Batch 40, Loss: 0.1531071130739066\n",
      "Epoch 18, Batch 50, Loss: 0.1719941328743707\n",
      "Epoch 18, Batch 60, Loss: 0.05064634001855694\n",
      "Epoch 18, Batch 70, Loss: 0.11480402662797519\n",
      "Epoch 18, Batch 80, Loss: 0.043508379918716\n",
      "Epoch 19, Batch 0, Loss: 0.025531609015038172\n",
      "Epoch 19, Batch 10, Loss: 0.20932788363332086\n",
      "Epoch 19, Batch 20, Loss: 0.3843305061597709\n",
      "Epoch 19, Batch 30, Loss: 0.12182523907051568\n",
      "Epoch 19, Batch 40, Loss: 0.043173839015245956\n",
      "Epoch 19, Batch 50, Loss: 0.27040518208669284\n",
      "Epoch 19, Batch 60, Loss: 0.0002573951255329546\n",
      "Epoch 19, Batch 70, Loss: 0.4140873881933185\n",
      "Epoch 19, Batch 80, Loss: 0.2506288617686367\n",
      "Epoch 20, Batch 0, Loss: 0.4168933650032338\n",
      "Epoch 20, Batch 10, Loss: 0.13340032805213559\n",
      "Epoch 20, Batch 20, Loss: 0.12651418140215512\n",
      "Epoch 20, Batch 30, Loss: 0.312894744506029\n",
      "Epoch 20, Batch 40, Loss: 0.05830634203781035\n",
      "Epoch 20, Batch 50, Loss: 4.6281593028930126e-09\n",
      "Epoch 20, Batch 60, Loss: 0.09856876491283889\n",
      "Epoch 20, Batch 70, Loss: 0.232717265873447\n",
      "Epoch 20, Batch 80, Loss: 0.2513592834927997\n",
      "Epoch 21, Batch 0, Loss: 0.1333909395855236\n",
      "Epoch 21, Batch 10, Loss: 0.3275682525089899\n",
      "Epoch 21, Batch 20, Loss: 0.007598928853036142\n",
      "Epoch 21, Batch 30, Loss: 0.030741008511096048\n",
      "Epoch 21, Batch 40, Loss: 0.18916987331430182\n",
      "Epoch 21, Batch 50, Loss: 0.25142303278216366\n",
      "Epoch 21, Batch 60, Loss: 0.1104179856318381\n",
      "Epoch 21, Batch 70, Loss: 0.09291245585719955\n",
      "Epoch 21, Batch 80, Loss: 0.2264834808539538\n",
      "Epoch 22, Batch 0, Loss: 0.032277920488742555\n",
      "Epoch 22, Batch 10, Loss: 0.006531493055726281\n",
      "Epoch 22, Batch 20, Loss: 0.16563265717649067\n",
      "Epoch 22, Batch 30, Loss: 0.10152073216701696\n",
      "Epoch 22, Batch 40, Loss: 0.0510551928017717\n",
      "Epoch 22, Batch 50, Loss: 0.001554833618792231\n",
      "Epoch 22, Batch 60, Loss: 0.48060265098508326\n",
      "Epoch 22, Batch 70, Loss: 0.08868356108227254\n",
      "Epoch 22, Batch 80, Loss: 0.05653976510849197\n",
      "Epoch 23, Batch 0, Loss: 0.1808409078266694\n",
      "Epoch 23, Batch 10, Loss: 0.43397400154948323\n",
      "Epoch 23, Batch 20, Loss: 0.2684825342601953\n",
      "Epoch 23, Batch 30, Loss: 0.1082655076432778\n",
      "Epoch 23, Batch 40, Loss: 0.0006830356486747499\n",
      "Epoch 23, Batch 50, Loss: 0.2766587777943482\n",
      "Epoch 23, Batch 60, Loss: 8.300263569387653e-06\n",
      "Epoch 23, Batch 70, Loss: 0.06184674309476619\n",
      "Epoch 23, Batch 80, Loss: 0.00028976457731162523\n",
      "Epoch 24, Batch 0, Loss: 5.177740261365601e-07\n",
      "Epoch 24, Batch 10, Loss: 0.16125654290090302\n",
      "Epoch 24, Batch 20, Loss: 0.129400228935\n",
      "Epoch 24, Batch 30, Loss: 0.062111980307510486\n",
      "Epoch 24, Batch 40, Loss: 0.061986794134715265\n",
      "Epoch 24, Batch 50, Loss: 0.10978722960636034\n",
      "Epoch 24, Batch 60, Loss: 1.2429137881803945e-06\n",
      "Epoch 24, Batch 70, Loss: 0.2051250376006962\n",
      "Epoch 24, Batch 80, Loss: 0.06856018974311112\n",
      "Epoch 25, Batch 0, Loss: 0.1771214117446007\n",
      "Epoch 25, Batch 10, Loss: 1.611836035468223e-07\n",
      "Epoch 25, Batch 20, Loss: 0.15213414902806274\n",
      "Epoch 25, Batch 30, Loss: 0.008320163959585555\n",
      "Epoch 25, Batch 40, Loss: 0.15869792592966456\n",
      "Epoch 25, Batch 50, Loss: 0.15315658587380765\n",
      "Epoch 25, Batch 60, Loss: 0.061186916882392785\n",
      "Epoch 25, Batch 70, Loss: 0.21000499665703215\n",
      "Epoch 25, Batch 80, Loss: 0.0024440245117163333\n",
      "Epoch 26, Batch 0, Loss: 0.11189764451048853\n",
      "Epoch 26, Batch 10, Loss: 4.4097886581713245e-05\n",
      "Epoch 26, Batch 20, Loss: 0.0016844758569775388\n",
      "Epoch 26, Batch 30, Loss: 0.0802873855069407\n",
      "Epoch 26, Batch 40, Loss: 0.11534218350114929\n",
      "Epoch 26, Batch 50, Loss: 1.4265004296644104e-08\n",
      "Epoch 26, Batch 60, Loss: 0.07348113942056006\n",
      "Epoch 26, Batch 70, Loss: 5.211983338620102e-11\n",
      "Epoch 26, Batch 80, Loss: 0.01115563839122624\n",
      "Epoch 27, Batch 0, Loss: 0.00200774185328271\n",
      "Epoch 27, Batch 10, Loss: 0.04306280844495595\n",
      "Epoch 27, Batch 20, Loss: 0.14751497633646432\n",
      "Epoch 27, Batch 30, Loss: 0.061508403881661455\n",
      "Epoch 27, Batch 40, Loss: 0.07043318780380822\n",
      "Epoch 27, Batch 50, Loss: 0.12362569707928239\n",
      "Epoch 27, Batch 60, Loss: 0.0011158546738778345\n",
      "Epoch 27, Batch 70, Loss: 0.0052174270513550705\n",
      "Epoch 27, Batch 80, Loss: 0.016840841764914048\n",
      "Epoch 28, Batch 0, Loss: 0.23787173170255915\n",
      "Epoch 28, Batch 10, Loss: 5.391324053801182e-10\n",
      "Epoch 28, Batch 20, Loss: 0.11962893418586619\n",
      "Epoch 28, Batch 30, Loss: 1.7521399292835977e-11\n",
      "Epoch 28, Batch 40, Loss: 0.0009474604595959282\n",
      "Epoch 28, Batch 50, Loss: 0.13834404826612146\n",
      "Epoch 28, Batch 60, Loss: 0.02970756129287006\n",
      "Epoch 28, Batch 70, Loss: 0.0828377736969636\n",
      "Epoch 28, Batch 80, Loss: 0.1248256700586879\n",
      "Epoch 29, Batch 0, Loss: 0.03762681619745538\n",
      "Epoch 29, Batch 10, Loss: 2.3766073971592055e-09\n",
      "Epoch 29, Batch 20, Loss: 0.22958731675014718\n",
      "Epoch 29, Batch 30, Loss: 9.979779811019332e-05\n",
      "Epoch 29, Batch 40, Loss: 2.962929061295378e-07\n",
      "Epoch 29, Batch 50, Loss: 0.13491734069237657\n",
      "Epoch 29, Batch 60, Loss: 0.14187159698323304\n",
      "Epoch 29, Batch 70, Loss: 0.14059405808279743\n",
      "Epoch 29, Batch 80, Loss: 7.0831517430575146e-06\n",
      "Epoch 30, Batch 0, Loss: 1.2802916507043733e-07\n",
      "Epoch 30, Batch 10, Loss: 0.028194996376247023\n",
      "Epoch 30, Batch 20, Loss: 0.00039709797929067065\n",
      "Epoch 30, Batch 30, Loss: 1.3424222642406666e-05\n",
      "Epoch 30, Batch 40, Loss: 0.165220249448478\n",
      "Epoch 30, Batch 50, Loss: 0.011481108235238738\n",
      "Epoch 30, Batch 60, Loss: 0.02538972186759067\n",
      "Epoch 30, Batch 70, Loss: 7.443867008822134e-05\n",
      "Epoch 30, Batch 80, Loss: 0.00017693036683328041\n",
      "Epoch 31, Batch 0, Loss: 0.04695073778663283\n",
      "Epoch 31, Batch 10, Loss: 0.07473240420083761\n",
      "Epoch 31, Batch 20, Loss: 0.09924775981855738\n",
      "Epoch 31, Batch 30, Loss: 3.412807007019439e-05\n",
      "Epoch 31, Batch 40, Loss: 0.023437615235814932\n",
      "Epoch 31, Batch 50, Loss: 0.012004598641439072\n",
      "Epoch 31, Batch 60, Loss: 0.06610633211388312\n",
      "Epoch 31, Batch 70, Loss: 0.2456163439373981\n",
      "Epoch 31, Batch 80, Loss: 0.009090199471059395\n",
      "Epoch 32, Batch 0, Loss: 0.00012947312272413469\n",
      "Epoch 32, Batch 10, Loss: 1.5568827797077485e-06\n",
      "Epoch 32, Batch 20, Loss: 0.06767822698503982\n",
      "Epoch 32, Batch 30, Loss: 0.08304527817881488\n",
      "Epoch 32, Batch 40, Loss: 7.354349864864683e-10\n",
      "Epoch 32, Batch 50, Loss: 0.07271122142941992\n",
      "Epoch 32, Batch 60, Loss: 0.09179846339313794\n",
      "Epoch 32, Batch 70, Loss: 0.0001917563127934998\n",
      "Epoch 32, Batch 80, Loss: 0.13491996727202044\n",
      "Epoch 33, Batch 0, Loss: 5.1428208621077355e-08\n",
      "Epoch 33, Batch 10, Loss: 0.03945392721931635\n",
      "Epoch 33, Batch 20, Loss: 0.19104731750233622\n",
      "Epoch 33, Batch 30, Loss: 7.77286630142281e-06\n",
      "Epoch 33, Batch 40, Loss: 0.13493182185359828\n",
      "Epoch 33, Batch 50, Loss: 0.0361136037039287\n",
      "Epoch 33, Batch 60, Loss: 0.0012028913031025157\n",
      "Epoch 33, Batch 70, Loss: 0.009143801026537581\n",
      "Epoch 33, Batch 80, Loss: 5.9676082441594725e-05\n",
      "Epoch 34, Batch 0, Loss: 0.00048436609854316724\n",
      "Epoch 34, Batch 10, Loss: 0.0001498167946222618\n",
      "Epoch 34, Batch 20, Loss: 0.05815310293186563\n",
      "Epoch 34, Batch 30, Loss: 3.8715820015977784e-08\n",
      "Epoch 34, Batch 40, Loss: 5.673948208879825e-06\n",
      "Epoch 34, Batch 50, Loss: 4.073007778344672e-06\n",
      "Epoch 34, Batch 60, Loss: 0.021625197677577437\n",
      "Epoch 34, Batch 70, Loss: 0.13437888189430033\n",
      "Epoch 34, Batch 80, Loss: 0.10591798426107164\n",
      "Epoch 35, Batch 0, Loss: 0.014833571751989913\n",
      "Epoch 35, Batch 10, Loss: 4.883807603762569e-07\n",
      "Epoch 35, Batch 20, Loss: 0.1445057398894128\n",
      "Epoch 35, Batch 30, Loss: 0.10399116650311156\n",
      "Epoch 35, Batch 40, Loss: 0.1683636892706611\n",
      "Epoch 35, Batch 50, Loss: 4.7827431914324735e-06\n",
      "Epoch 35, Batch 60, Loss: 0.021747725808520694\n",
      "Epoch 35, Batch 70, Loss: 0.22174874080261556\n",
      "Epoch 35, Batch 80, Loss: 0.22186796059959946\n",
      "Epoch 36, Batch 0, Loss: 0.0002579554971411699\n",
      "Epoch 36, Batch 10, Loss: 0.03513715455522204\n",
      "Epoch 36, Batch 20, Loss: 0.013454617447375089\n",
      "Epoch 36, Batch 30, Loss: 0.01582329469939967\n",
      "Epoch 36, Batch 40, Loss: 0.012327869436984322\n",
      "Epoch 36, Batch 50, Loss: 6.702571397264437e-07\n",
      "Epoch 36, Batch 60, Loss: 0.21033440595549596\n",
      "Epoch 36, Batch 70, Loss: 3.173549948663798e-07\n",
      "Epoch 36, Batch 80, Loss: 0.03660215816868667\n",
      "Epoch 37, Batch 0, Loss: 0.06057818000700819\n",
      "Epoch 37, Batch 10, Loss: 0.17840193355509518\n",
      "Epoch 37, Batch 20, Loss: 1.1544584732626483e-15\n",
      "Epoch 37, Batch 30, Loss: 0.06987889920697352\n",
      "Epoch 37, Batch 40, Loss: 8.281895821835099e-06\n",
      "Epoch 37, Batch 50, Loss: 4.042874895434076e-06\n",
      "Epoch 37, Batch 60, Loss: 0.2569550672514664\n",
      "Epoch 37, Batch 70, Loss: 0.11062643884719683\n",
      "Epoch 37, Batch 80, Loss: 4.495029799442744e-06\n",
      "Epoch 38, Batch 0, Loss: 0.060903127360240124\n",
      "Epoch 38, Batch 10, Loss: 1.5192948574048058e-09\n",
      "Epoch 38, Batch 20, Loss: 0.06007094251989565\n",
      "Epoch 38, Batch 30, Loss: 3.5010752307289216e-09\n",
      "Epoch 38, Batch 40, Loss: 0.0022641034725465036\n",
      "Epoch 38, Batch 50, Loss: 0.09247057673556078\n",
      "Epoch 38, Batch 60, Loss: 0.04727015304763899\n",
      "Epoch 38, Batch 70, Loss: 0.021919072380910544\n",
      "Epoch 38, Batch 80, Loss: 0.043772659766398846\n",
      "Epoch 39, Batch 0, Loss: 2.6592483509008205e-11\n",
      "Epoch 39, Batch 10, Loss: 0.040828140775748985\n",
      "Epoch 39, Batch 20, Loss: 0.03998294837150517\n",
      "Epoch 39, Batch 30, Loss: 4.6885515292162814e-08\n",
      "Epoch 39, Batch 40, Loss: 5.809280009992355e-11\n",
      "Epoch 39, Batch 50, Loss: 4.038941125501101e-08\n",
      "Epoch 39, Batch 60, Loss: 0.009357557783036955\n",
      "Epoch 39, Batch 70, Loss: 3.400447949752457e-11\n",
      "Epoch 39, Batch 80, Loss: 0.015459619942294774\n",
      "Epoch 40, Batch 0, Loss: 9.006223215831566e-06\n",
      "Epoch 40, Batch 10, Loss: 1.4566174770015325e-05\n",
      "Epoch 40, Batch 20, Loss: 4.534029402188185e-13\n",
      "Epoch 40, Batch 30, Loss: 5.311914103384307e-13\n",
      "Epoch 40, Batch 40, Loss: 7.0629576636590435e-06\n",
      "Epoch 40, Batch 50, Loss: 0.0002180399385213419\n",
      "Epoch 40, Batch 60, Loss: 1.1076870565425086e-06\n",
      "Epoch 40, Batch 70, Loss: 0.028716249414296867\n",
      "Epoch 40, Batch 80, Loss: 9.949898936899453e-05\n",
      "Epoch 41, Batch 0, Loss: 2.0366538320841525e-12\n",
      "Epoch 41, Batch 10, Loss: 0.020286787769445203\n",
      "Epoch 41, Batch 20, Loss: 1.8845283909491136e-09\n",
      "Epoch 41, Batch 30, Loss: 1.144917494144704e-16\n",
      "Epoch 41, Batch 40, Loss: 1.3955238622963451e-05\n",
      "Epoch 41, Batch 50, Loss: 0.028518486862542464\n",
      "Epoch 41, Batch 60, Loss: 3.5679306192828005e-05\n",
      "Epoch 41, Batch 70, Loss: 5.885135592984838e-11\n",
      "Epoch 41, Batch 80, Loss: 0.007984814596789862\n",
      "Epoch 42, Batch 0, Loss: 5.791496752201563e-11\n",
      "Epoch 42, Batch 10, Loss: 0.1344137361963663\n",
      "Epoch 42, Batch 20, Loss: 2.5153490401663783e-17\n",
      "Epoch 42, Batch 30, Loss: 0.0001711475759152295\n",
      "Epoch 42, Batch 40, Loss: 0.0064960608622697345\n",
      "Epoch 42, Batch 50, Loss: 2.230231122874594e-05\n",
      "Epoch 42, Batch 60, Loss: 0.08928827175215792\n",
      "Epoch 42, Batch 70, Loss: 2.923549984295446e-10\n",
      "Epoch 42, Batch 80, Loss: 4.2274086294401145e-05\n",
      "Epoch 43, Batch 0, Loss: 0.0020387889288760465\n",
      "Epoch 43, Batch 10, Loss: 0.01901664069824174\n",
      "Epoch 43, Batch 20, Loss: 0.004966554064319018\n",
      "Epoch 43, Batch 30, Loss: 0.02706954555991973\n",
      "Epoch 43, Batch 40, Loss: 2.089100771383783e-08\n",
      "Epoch 43, Batch 50, Loss: 0.007956627068786703\n",
      "Epoch 43, Batch 60, Loss: 1.1129670483615371e-05\n",
      "Epoch 43, Batch 70, Loss: 0.0008635503081731921\n",
      "Epoch 43, Batch 80, Loss: 0.02861691822719015\n",
      "Epoch 44, Batch 0, Loss: 0.001145257822008258\n",
      "Epoch 44, Batch 10, Loss: 7.848366836020204e-06\n",
      "Epoch 44, Batch 20, Loss: 0.022216406114181304\n",
      "Epoch 44, Batch 30, Loss: 9.692691780368718e-06\n",
      "Epoch 44, Batch 40, Loss: 0.05295372788509281\n",
      "Epoch 44, Batch 50, Loss: 0.13491965390177574\n",
      "Epoch 44, Batch 60, Loss: 1.2497650485071959e-12\n",
      "Epoch 44, Batch 70, Loss: 5.345211608430404e-07\n",
      "Epoch 44, Batch 80, Loss: 3.0588379053095165e-13\n",
      "Epoch 45, Batch 0, Loss: 2.862293735361742e-17\n",
      "Epoch 45, Batch 10, Loss: 4.369870786119208e-12\n",
      "Epoch 45, Batch 20, Loss: 1.9859479413281515e-05\n",
      "Epoch 45, Batch 30, Loss: 5.964409526379422e-12\n",
      "Epoch 45, Batch 40, Loss: 4.0991826004543993e-11\n",
      "Epoch 45, Batch 50, Loss: 7.624005594001176e-13\n",
      "Epoch 45, Batch 60, Loss: 1.2183729258619586e-10\n",
      "Epoch 45, Batch 70, Loss: 5.091317987617456e-07\n",
      "Epoch 45, Batch 80, Loss: 4.597884573079201e-15\n",
      "Epoch 46, Batch 0, Loss: 4.493742259195947e-06\n",
      "Epoch 46, Batch 10, Loss: 2.1127672752572335e-10\n",
      "Epoch 46, Batch 20, Loss: 2.792144768320949e-09\n",
      "Epoch 46, Batch 30, Loss: 0.033603125950519605\n",
      "Epoch 46, Batch 40, Loss: 0.049739120464175335\n",
      "Epoch 46, Batch 50, Loss: 0.012334733412544237\n",
      "Epoch 46, Batch 60, Loss: 3.908689730787896e-10\n",
      "Epoch 46, Batch 70, Loss: 2.1224673694496963e-06\n",
      "Epoch 46, Batch 80, Loss: 0.04589552685207467\n",
      "Epoch 47, Batch 0, Loss: 1.946336673044441e-11\n",
      "Epoch 47, Batch 10, Loss: 3.398311788475638e-08\n",
      "Epoch 47, Batch 20, Loss: 9.454161627417873e-11\n",
      "Epoch 47, Batch 30, Loss: 2.5359410834195963e-11\n",
      "Epoch 47, Batch 40, Loss: 9.96722419262027e-10\n",
      "Epoch 47, Batch 50, Loss: 1.68194476807867e-08\n",
      "Epoch 47, Batch 60, Loss: 0.015003238259762495\n",
      "Epoch 47, Batch 70, Loss: 4.5132387413228926e-13\n",
      "Epoch 47, Batch 80, Loss: 0.05487855471429133\n",
      "Epoch 48, Batch 0, Loss: 1.7381929229290928e-15\n",
      "Epoch 48, Batch 10, Loss: 6.2365746297634635e-12\n",
      "Epoch 48, Batch 20, Loss: 8.3312479954348e-08\n",
      "Epoch 48, Batch 30, Loss: 0.03019421171660029\n",
      "Epoch 48, Batch 40, Loss: 0.02853250759927128\n",
      "Epoch 48, Batch 50, Loss: 1.5600699264853126e-08\n",
      "Epoch 48, Batch 60, Loss: 5.839079220141869e-15\n",
      "Epoch 48, Batch 70, Loss: 0.03436539951894543\n",
      "Epoch 48, Batch 80, Loss: 0.02047736747255693\n",
      "Epoch 49, Batch 0, Loss: 0.13512119466569086\n",
      "Epoch 49, Batch 10, Loss: 6.553191954374836e-06\n",
      "Epoch 49, Batch 20, Loss: 2.642000398585771e-10\n",
      "Epoch 49, Batch 30, Loss: 1.1262760916167774e-05\n",
      "Epoch 49, Batch 40, Loss: 0.002259736610798921\n",
      "Epoch 49, Batch 50, Loss: 0.07039782794894114\n",
      "Epoch 49, Batch 60, Loss: 0.03946074503750701\n",
      "Epoch 49, Batch 70, Loss: 0.006685773908677385\n",
      "Epoch 49, Batch 80, Loss: 0.0006083351139824373\n",
      "Epoch 50, Batch 0, Loss: 0.023349464260717618\n",
      "Epoch 50, Batch 10, Loss: 0.001583889797528792\n",
      "Epoch 50, Batch 20, Loss: 3.2986329787348786e-08\n",
      "Epoch 50, Batch 30, Loss: 0.14185022623810842\n",
      "Epoch 50, Batch 40, Loss: 8.250865268854704e-14\n",
      "Epoch 50, Batch 50, Loss: 0.008434836826693664\n",
      "Epoch 50, Batch 60, Loss: 0.13578787543326598\n",
      "Epoch 50, Batch 70, Loss: 0.0008929375480612521\n",
      "Epoch 50, Batch 80, Loss: 1.3878655169564874e-14\n",
      "Epoch 51, Batch 0, Loss: 0.0004110322264687329\n",
      "Epoch 51, Batch 10, Loss: 5.476297890091824e-07\n",
      "Epoch 51, Batch 20, Loss: 1.7327822721034442e-10\n",
      "Epoch 51, Batch 30, Loss: 0.059469178375839475\n",
      "Epoch 51, Batch 40, Loss: 1.713290967461409e-13\n",
      "Epoch 51, Batch 50, Loss: 5.201164192086106e-09\n",
      "Epoch 51, Batch 60, Loss: 9.310097835137326e-07\n",
      "Epoch 51, Batch 70, Loss: 4.648677481100713e-09\n",
      "Epoch 51, Batch 80, Loss: 0.04346770243049535\n",
      "Epoch 52, Batch 0, Loss: 4.346627227228043e-12\n",
      "Epoch 52, Batch 10, Loss: 0.0001684497407934822\n",
      "Epoch 52, Batch 20, Loss: 9.213063252897105e-10\n",
      "Epoch 52, Batch 30, Loss: 4.7134968934853826e-07\n",
      "Epoch 52, Batch 40, Loss: 4.485324440803739e-12\n",
      "Epoch 52, Batch 50, Loss: 0.006165916730341637\n",
      "Epoch 52, Batch 60, Loss: 0.01190001476889744\n",
      "Epoch 52, Batch 70, Loss: 0.0005993673088741182\n",
      "Epoch 52, Batch 80, Loss: 3.732921311442282e-09\n",
      "Epoch 53, Batch 0, Loss: 9.060073397989126e-08\n",
      "Epoch 53, Batch 10, Loss: 7.168420528978867e-07\n",
      "Epoch 53, Batch 20, Loss: 2.316627792724359e-12\n",
      "Epoch 53, Batch 30, Loss: 0.0072513313893599355\n",
      "Epoch 53, Batch 40, Loss: 1.4243654740051883e-09\n",
      "Epoch 53, Batch 50, Loss: 0.0001131571911060431\n",
      "Epoch 53, Batch 60, Loss: 7.630378974966995e-12\n",
      "Epoch 53, Batch 70, Loss: 1.8035843614158116e-08\n",
      "Epoch 53, Batch 80, Loss: 0.0005054785744470664\n",
      "Epoch 54, Batch 0, Loss: 4.95744713050257e-06\n",
      "Epoch 54, Batch 10, Loss: 9.447892127566547e-12\n",
      "Epoch 54, Batch 20, Loss: 0.04590431667745425\n",
      "Epoch 54, Batch 30, Loss: 1.8769753146187652e-11\n",
      "Epoch 54, Batch 40, Loss: 9.052232514261955e-06\n",
      "Epoch 54, Batch 50, Loss: 1.870443645983703e-11\n",
      "Epoch 54, Batch 60, Loss: 0.014880397895134562\n",
      "Epoch 54, Batch 70, Loss: 0.03129760024487054\n",
      "Epoch 54, Batch 80, Loss: 4.429178069209293e-05\n",
      "Epoch 55, Batch 0, Loss: 2.3053416265329302e-05\n",
      "Epoch 55, Batch 10, Loss: 9.760948363934011e-11\n",
      "Epoch 55, Batch 20, Loss: 3.3142821890029616e-11\n",
      "Epoch 55, Batch 30, Loss: 1.4642049130212563e-10\n",
      "Epoch 55, Batch 40, Loss: 0.008507999797073717\n",
      "Epoch 55, Batch 50, Loss: 0.0001287090518349967\n",
      "Epoch 55, Batch 60, Loss: 2.8019102104406145e-09\n",
      "Epoch 55, Batch 70, Loss: 8.819842228767669e-08\n",
      "Epoch 55, Batch 80, Loss: 6.218784959771745e-07\n",
      "Epoch 56, Batch 0, Loss: 0.046076028274945176\n",
      "Epoch 56, Batch 10, Loss: 2.0282697197429876e-09\n",
      "Epoch 56, Batch 20, Loss: 0.12002481685545296\n",
      "Epoch 56, Batch 30, Loss: 5.108399485671509e-07\n",
      "Epoch 56, Batch 40, Loss: 0.059949324486224594\n",
      "Epoch 56, Batch 50, Loss: 0.032528635733203436\n",
      "Epoch 56, Batch 60, Loss: 3.57101913203319e-05\n",
      "Epoch 56, Batch 70, Loss: 6.490290081524456e-12\n",
      "Epoch 56, Batch 80, Loss: 1.597265149628968e-08\n",
      "Epoch 57, Batch 0, Loss: 6.555376964450936e-09\n",
      "Epoch 57, Batch 10, Loss: 1.82145964977565e-17\n",
      "Epoch 57, Batch 20, Loss: 1.015645379649779e-09\n",
      "Epoch 57, Batch 30, Loss: 0.024465770148679834\n",
      "Epoch 57, Batch 40, Loss: 8.512045675620437e-06\n",
      "Epoch 57, Batch 50, Loss: 6.418476861114204e-17\n",
      "Epoch 57, Batch 60, Loss: 1.6080951705094642e-11\n",
      "Epoch 57, Batch 70, Loss: 0.05806906034821521\n",
      "Epoch 57, Batch 80, Loss: 5.156709014808866e-08\n",
      "Epoch 58, Batch 0, Loss: 8.872395533652259e-10\n",
      "Epoch 58, Batch 10, Loss: 8.645390208483526e-09\n",
      "Epoch 58, Batch 20, Loss: 1.621966450038339e-16\n",
      "Epoch 58, Batch 30, Loss: 4.5970172113385647e-17\n",
      "Epoch 58, Batch 40, Loss: 2.450382389778564e-06\n",
      "Epoch 58, Batch 50, Loss: 0.07237350418991603\n",
      "Epoch 58, Batch 60, Loss: 0.15188135092884103\n",
      "Epoch 58, Batch 70, Loss: 2.6041481117807424e-09\n",
      "Epoch 58, Batch 80, Loss: 4.336808689942031e-17\n",
      "Epoch 59, Batch 0, Loss: 1.820592288038021e-15\n",
      "Epoch 59, Batch 10, Loss: 0.004031929262029607\n",
      "Epoch 59, Batch 20, Loss: 8.34226958547695e-11\n",
      "Epoch 59, Batch 30, Loss: 6.208956960171532e-13\n",
      "Epoch 59, Batch 40, Loss: 4.205413346829103e-08\n",
      "Epoch 59, Batch 50, Loss: 4.842848905781407e-09\n",
      "Epoch 59, Batch 60, Loss: 0.020280397783755316\n",
      "Epoch 59, Batch 70, Loss: 6.489818262105088e-10\n",
      "Epoch 59, Batch 80, Loss: 5.355106963150344e-11\n",
      "Epoch 60, Batch 0, Loss: 0.004234155135323782\n",
      "Epoch 60, Batch 10, Loss: 4.1757048736688186e-10\n",
      "Epoch 60, Batch 20, Loss: 2.1514560966699593e-13\n",
      "Epoch 60, Batch 30, Loss: 8.5943617498672e-10\n",
      "Epoch 60, Batch 40, Loss: 3.4341284308376176e-08\n",
      "Epoch 60, Batch 50, Loss: 4.469642717087591e-06\n",
      "Epoch 60, Batch 60, Loss: 1.158443133256233e-12\n",
      "Epoch 60, Batch 70, Loss: 7.302197798355629e-06\n",
      "Epoch 60, Batch 80, Loss: 2.77404338075613e-10\n",
      "Epoch 61, Batch 0, Loss: 3.606101648273253e-10\n",
      "Epoch 61, Batch 10, Loss: 3.683922237442635e-11\n",
      "Epoch 61, Batch 20, Loss: 0.020226535257785075\n",
      "Epoch 61, Batch 30, Loss: 1.551408563687167e-10\n",
      "Epoch 61, Batch 40, Loss: 0.008479231297456456\n",
      "Epoch 61, Batch 50, Loss: 1.805083145460621e-08\n",
      "Epoch 61, Batch 60, Loss: 5.2041704279304236e-18\n",
      "Epoch 61, Batch 70, Loss: 0.0012729507672228097\n",
      "Epoch 61, Batch 80, Loss: 8.92376990763452e-10\n",
      "Epoch 62, Batch 0, Loss: 3.0327753815948467e-07\n",
      "Epoch 62, Batch 10, Loss: 9.992748470691763e-09\n",
      "Epoch 62, Batch 20, Loss: -0.0\n",
      "Epoch 62, Batch 30, Loss: 5.453884070864385e-10\n",
      "Epoch 62, Batch 40, Loss: 4.515859132381226e-06\n",
      "Epoch 62, Batch 50, Loss: 1.125002125883952e-10\n",
      "Epoch 62, Batch 60, Loss: 2.4458647501323235e-05\n",
      "Epoch 62, Batch 70, Loss: 2.5944341797553035e-09\n",
      "Epoch 62, Batch 80, Loss: 1.3218444444787584e-10\n",
      "Epoch 63, Batch 0, Loss: 1.8301332671555744e-16\n",
      "Epoch 63, Batch 10, Loss: 1.7325935327667295e-05\n",
      "Epoch 63, Batch 20, Loss: 2.3045801378358682e-15\n",
      "Epoch 63, Batch 30, Loss: 0.04483956409511463\n",
      "Epoch 63, Batch 40, Loss: 0.0021308280855711972\n",
      "Epoch 63, Batch 50, Loss: 3.8341597215298276e-05\n",
      "Epoch 63, Batch 60, Loss: 0.0226024114727491\n",
      "Epoch 63, Batch 70, Loss: 2.906454190851809e-06\n",
      "Epoch 63, Batch 80, Loss: 0.004342132255784909\n",
      "Epoch 64, Batch 0, Loss: 1.0304656434513206e-12\n",
      "Epoch 64, Batch 10, Loss: 2.608687572374036e-12\n",
      "Epoch 64, Batch 20, Loss: 1.1520201629319023e-10\n",
      "Epoch 64, Batch 30, Loss: 5.675017837657239e-07\n",
      "Epoch 64, Batch 40, Loss: 4.271837226181095e-12\n",
      "Epoch 64, Batch 50, Loss: 2.7435566214907365e-06\n",
      "Epoch 64, Batch 60, Loss: 3.3740371607750283e-16\n",
      "Epoch 64, Batch 70, Loss: 7.047790612999401e-09\n",
      "Epoch 64, Batch 80, Loss: 0.014448326689339925\n",
      "Epoch 65, Batch 0, Loss: 4.769988045451145e-09\n",
      "Epoch 65, Batch 10, Loss: 6.772381715915212e-11\n",
      "Epoch 65, Batch 20, Loss: 0.006120776735712106\n",
      "Epoch 65, Batch 30, Loss: 5.230191280070385e-16\n",
      "Epoch 65, Batch 40, Loss: 1.056531598463076e-12\n",
      "Epoch 65, Batch 50, Loss: 1.2239614892547368e-08\n",
      "Epoch 65, Batch 60, Loss: 8.02573638021483e-05\n",
      "Epoch 65, Batch 70, Loss: 1.787570528916336e-05\n",
      "Epoch 65, Batch 80, Loss: 5.3221316243001945e-15\n",
      "Epoch 66, Batch 0, Loss: 6.671278632940422e-07\n",
      "Epoch 66, Batch 10, Loss: 7.736866702856933e-16\n",
      "Epoch 66, Batch 20, Loss: -0.0\n",
      "Epoch 66, Batch 30, Loss: 2.888314587501488e-16\n",
      "Epoch 66, Batch 40, Loss: 2.759854946490187e-10\n",
      "Epoch 66, Batch 50, Loss: 2.6856459850987516e-11\n",
      "Epoch 66, Batch 60, Loss: 9.540979117872554e-17\n",
      "Epoch 66, Batch 70, Loss: 0.0019253824876994663\n",
      "Epoch 66, Batch 80, Loss: 4.239068658695135e-05\n",
      "Epoch 67, Batch 0, Loss: 1.584825350489258e-10\n",
      "Epoch 67, Batch 10, Loss: 3.730228801181939e-12\n",
      "Epoch 67, Batch 20, Loss: 0.028986744951408547\n",
      "Epoch 67, Batch 30, Loss: 8.673617379884037e-19\n",
      "Epoch 67, Batch 40, Loss: 1.0872756165197637e-05\n",
      "Epoch 67, Batch 50, Loss: 3.6814032836658604e-10\n",
      "Epoch 67, Batch 60, Loss: 3.549244231850136e-15\n",
      "Epoch 67, Batch 70, Loss: 4.7395898172280995e-07\n",
      "Epoch 67, Batch 80, Loss: 0.06845091514408785\n",
      "Epoch 68, Batch 0, Loss: 3.6806661286070223e-09\n",
      "Epoch 68, Batch 10, Loss: 1.6148844417811858e-12\n",
      "Epoch 68, Batch 20, Loss: 0.013245982180089268\n",
      "Epoch 68, Batch 30, Loss: 0.022336386052654776\n",
      "Epoch 68, Batch 40, Loss: 2.097924265405429e-12\n",
      "Epoch 68, Batch 50, Loss: 0.027706301416583715\n",
      "Epoch 68, Batch 60, Loss: 8.062534967923885e-05\n",
      "Epoch 68, Batch 70, Loss: 2.6888213877640584e-17\n",
      "Epoch 68, Batch 80, Loss: 0.03897447634743819\n",
      "Epoch 69, Batch 0, Loss: 9.17376417987502e-13\n",
      "Epoch 69, Batch 10, Loss: 1.3449826152772015e-07\n",
      "Epoch 69, Batch 20, Loss: 2.8648953074301545e-09\n",
      "Epoch 69, Batch 30, Loss: 1.1981896068891178e-07\n",
      "Epoch 69, Batch 40, Loss: 2.0023202784928328e-08\n",
      "Epoch 69, Batch 50, Loss: 1.225744788847884e-09\n",
      "Epoch 69, Batch 60, Loss: 2.1832956050680923e-10\n",
      "Epoch 69, Batch 70, Loss: 3.2402032446100495e-14\n",
      "Epoch 69, Batch 80, Loss: 9.28259205623468e-14\n",
      "Epoch 70, Batch 0, Loss: 1.1258215336253488e-08\n",
      "Epoch 70, Batch 10, Loss: 6.902888403709018e-07\n",
      "Epoch 70, Batch 20, Loss: 9.58434720477303e-16\n",
      "Epoch 70, Batch 30, Loss: 1.1299200061981865e-10\n",
      "Epoch 70, Batch 40, Loss: 3.3006002465542963e-06\n",
      "Epoch 70, Batch 50, Loss: 0.04929861761994337\n",
      "Epoch 70, Batch 60, Loss: 0.12253565522507545\n",
      "Epoch 70, Batch 70, Loss: 3.3680596217725255e-11\n",
      "Epoch 70, Batch 80, Loss: 1.6523764923850554e-07\n",
      "Epoch 71, Batch 0, Loss: 1.560383766641295e-15\n",
      "Epoch 71, Batch 10, Loss: 6.26301540038268e-05\n",
      "Epoch 71, Batch 20, Loss: 8.667600344031486e-09\n",
      "Epoch 71, Batch 30, Loss: 3.9811638346025767e-07\n",
      "Epoch 71, Batch 40, Loss: 2.3432353570472785e-06\n",
      "Epoch 71, Batch 50, Loss: 0.06032258639065674\n",
      "Epoch 71, Batch 60, Loss: 6.759192291973273e-07\n",
      "Epoch 71, Batch 70, Loss: 4.903097602570062e-06\n",
      "Epoch 71, Batch 80, Loss: 1.5965668923743368e-11\n",
      "Epoch 72, Batch 0, Loss: -0.0\n",
      "Epoch 72, Batch 10, Loss: 5.277255057509973e-11\n",
      "Epoch 72, Batch 20, Loss: 7.648923562334372e-11\n",
      "Epoch 72, Batch 30, Loss: 5.3319225003160056e-06\n",
      "Epoch 72, Batch 40, Loss: 9.946991182670444e-11\n",
      "Epoch 72, Batch 50, Loss: 1.7347234759768075e-18\n",
      "Epoch 72, Batch 60, Loss: 2.5727001733872455e-08\n",
      "Epoch 72, Batch 70, Loss: 5.787991613990774e-13\n",
      "Epoch 72, Batch 80, Loss: 7.975025325941776e-05\n",
      "Epoch 73, Batch 0, Loss: 8.67361737988413e-17\n",
      "Epoch 73, Batch 10, Loss: 1.7607443281164974e-16\n",
      "Epoch 73, Batch 20, Loss: 3.5491281785360354e-07\n",
      "Epoch 73, Batch 30, Loss: 1.8388036551286382e-05\n",
      "Epoch 73, Batch 40, Loss: 4.372711635504056e-11\n",
      "Epoch 73, Batch 50, Loss: 9.765351047150506e-10\n",
      "Epoch 73, Batch 60, Loss: 0.015008972608661178\n",
      "Epoch 73, Batch 70, Loss: 1.541371197374824e-13\n",
      "Epoch 73, Batch 80, Loss: 6.289413434532194e-14\n",
      "Epoch 74, Batch 0, Loss: 5.115814469106782e-09\n",
      "Epoch 74, Batch 10, Loss: 0.011395309072162059\n",
      "Epoch 74, Batch 20, Loss: 0.04807680593040685\n",
      "Epoch 74, Batch 30, Loss: 3.281121010587667e-10\n",
      "Epoch 74, Batch 40, Loss: -0.0\n",
      "Epoch 74, Batch 50, Loss: 2.546184450111267e-09\n",
      "Epoch 74, Batch 60, Loss: 5.481119030908482e-14\n",
      "Epoch 74, Batch 70, Loss: 5.838944145517161e-09\n",
      "Epoch 74, Batch 80, Loss: 2.343351207530297e-14\n",
      "Epoch 75, Batch 0, Loss: 1.5298592391167345e-09\n",
      "Epoch 75, Batch 10, Loss: 2.7284293727184287e-05\n",
      "Epoch 75, Batch 20, Loss: 1.474979342839527e-11\n",
      "Epoch 75, Batch 30, Loss: 1.0772985079195514e-07\n",
      "Epoch 75, Batch 40, Loss: 9.008837536166146e-07\n",
      "Epoch 75, Batch 50, Loss: 4.6837533851373915e-17\n",
      "Epoch 75, Batch 60, Loss: 1.0832879113531797e-09\n",
      "Epoch 75, Batch 70, Loss: 1.2620113287732236e-15\n",
      "Epoch 75, Batch 80, Loss: 1.2784183435310098e-12\n",
      "Epoch 76, Batch 0, Loss: 1.7347234759768075e-18\n",
      "Epoch 76, Batch 10, Loss: 1.2876440940909305e-06\n",
      "Epoch 76, Batch 20, Loss: 8.673617379884037e-19\n",
      "Epoch 76, Batch 30, Loss: 4.225612915142483e-14\n",
      "Epoch 76, Batch 40, Loss: 8.03558919659944e-09\n",
      "Epoch 76, Batch 50, Loss: 5.082644721672631e-10\n",
      "Epoch 76, Batch 60, Loss: 8.64149118936594e-07\n",
      "Epoch 76, Batch 70, Loss: 6.382865226237554e-07\n",
      "Epoch 76, Batch 80, Loss: 9.256777237185608e-05\n",
      "Epoch 77, Batch 0, Loss: -0.0\n",
      "Epoch 77, Batch 10, Loss: 6.775604383677868e-13\n",
      "Epoch 77, Batch 20, Loss: 1.827010764900421e-14\n",
      "Epoch 77, Batch 30, Loss: -0.0\n",
      "Epoch 77, Batch 40, Loss: 1.4390832276099513e-13\n",
      "Epoch 77, Batch 50, Loss: 3.6168984474117976e-16\n",
      "Epoch 77, Batch 60, Loss: -0.0\n",
      "Epoch 77, Batch 70, Loss: 0.006531831905513317\n",
      "Epoch 77, Batch 80, Loss: 7.368237964215628e-15\n",
      "Epoch 78, Batch 0, Loss: 1.703498453409596e-15\n",
      "Epoch 78, Batch 10, Loss: 4.2250319979313014e-07\n",
      "Epoch 78, Batch 20, Loss: 6.86239686485925e-08\n",
      "Epoch 78, Batch 30, Loss: 5.212770753262482e-09\n",
      "Epoch 78, Batch 40, Loss: 3.46944695195362e-17\n",
      "Epoch 78, Batch 50, Loss: 2.841680017239679e-12\n",
      "Epoch 78, Batch 60, Loss: 1.1065832293943514e-11\n",
      "Epoch 78, Batch 70, Loss: 4.4127571495827783e-10\n",
      "Epoch 78, Batch 80, Loss: 1.2463045171186926e-05\n",
      "Epoch 79, Batch 0, Loss: 8.601214541419288e-09\n",
      "Epoch 79, Batch 10, Loss: 4.5805373383194444e-15\n",
      "Epoch 79, Batch 20, Loss: 1.7347234759768075e-18\n",
      "Epoch 79, Batch 30, Loss: 1.6574313540795493e-10\n",
      "Epoch 79, Batch 40, Loss: 6.48637393850244e-13\n",
      "Epoch 79, Batch 50, Loss: 2.2748296302287714e-14\n",
      "Epoch 79, Batch 60, Loss: 4.356324329070933e-14\n",
      "Epoch 79, Batch 70, Loss: 1.0115632291068827e-12\n",
      "Epoch 79, Batch 80, Loss: 4.0419056990261045e-16\n",
      "Epoch 80, Batch 0, Loss: 1.7709011341084454e-13\n",
      "Epoch 80, Batch 10, Loss: 8.773259903306535e-12\n",
      "Epoch 80, Batch 20, Loss: 1.6774060688432445e-09\n",
      "Epoch 80, Batch 30, Loss: 4.570548803215994e-12\n",
      "Epoch 80, Batch 40, Loss: 6.93662704562233e-08\n",
      "Epoch 80, Batch 50, Loss: 1.1079331896525747e-13\n",
      "Epoch 80, Batch 60, Loss: 1.312335898119142e-08\n",
      "Epoch 80, Batch 70, Loss: 0.046637827367691634\n",
      "Epoch 80, Batch 80, Loss: 0.00960793679950843\n",
      "Epoch 81, Batch 0, Loss: 5.920575056101682e-05\n",
      "Epoch 81, Batch 10, Loss: 1.2872602289871722e-13\n",
      "Epoch 81, Batch 20, Loss: 1.2421487449750682e-14\n",
      "Epoch 81, Batch 30, Loss: 0.01678251785043566\n",
      "Epoch 81, Batch 40, Loss: 1.5757239610522164e-09\n",
      "Epoch 81, Batch 50, Loss: 1.4745149545803107e-16\n",
      "Epoch 81, Batch 60, Loss: 1.5633310634315142e-11\n",
      "Epoch 81, Batch 70, Loss: 2.164636465035103e-06\n",
      "Epoch 81, Batch 80, Loss: 0.03617207933958846\n",
      "Epoch 82, Batch 0, Loss: 2.0427222786924203e-06\n",
      "Epoch 82, Batch 10, Loss: 1.0036893517099714e-05\n",
      "Epoch 82, Batch 20, Loss: 2.62532280917845e-06\n",
      "Epoch 82, Batch 30, Loss: 4.333339242991627e-15\n",
      "Epoch 82, Batch 40, Loss: 6.60005818215367e-06\n",
      "Epoch 82, Batch 50, Loss: 1.566897653324857e-13\n",
      "Epoch 82, Batch 60, Loss: 5.91831693146922e-10\n",
      "Epoch 82, Batch 70, Loss: 1.4707679518998173e-13\n",
      "Epoch 82, Batch 80, Loss: 2.209237102117518e-10\n",
      "Epoch 83, Batch 0, Loss: 1.9949319973733642e-16\n",
      "Epoch 83, Batch 10, Loss: 5.121269501074496e-11\n",
      "Epoch 83, Batch 20, Loss: 2.2794266474341895e-15\n",
      "Epoch 83, Batch 30, Loss: 1.5872719805188034e-16\n",
      "Epoch 83, Batch 40, Loss: 9.743941764573186e-15\n",
      "Epoch 83, Batch 50, Loss: 1.908195823574492e-17\n",
      "Epoch 83, Batch 60, Loss: 3.7988032791028214e-06\n",
      "Epoch 83, Batch 70, Loss: 7.243771554877193e-14\n",
      "Epoch 83, Batch 80, Loss: 2.6020852139652114e-18\n",
      "Epoch 84, Batch 0, Loss: 1.4286608529177876e-10\n",
      "Epoch 84, Batch 10, Loss: 1.2358997844685733e-09\n",
      "Epoch 84, Batch 20, Loss: 0.0009615986487967714\n",
      "Epoch 84, Batch 30, Loss: 2.056176409746806e-13\n",
      "Epoch 84, Batch 40, Loss: 5.290906601729295e-17\n",
      "Epoch 84, Batch 50, Loss: 1.0853221183723705e-06\n",
      "Epoch 84, Batch 60, Loss: 0.09305762512966631\n",
      "Epoch 84, Batch 70, Loss: 1.4602121595481397e-13\n",
      "Epoch 84, Batch 80, Loss: 7.512623890134955e-09\n",
      "Epoch 85, Batch 0, Loss: 2.6020852139652114e-18\n",
      "Epoch 85, Batch 10, Loss: 1.273256673897582e-12\n",
      "Epoch 85, Batch 20, Loss: 7.812266459512781e-13\n",
      "Epoch 85, Batch 30, Loss: 1.1492543028347367e-15\n",
      "Epoch 85, Batch 40, Loss: 2.263814136149799e-16\n",
      "Epoch 85, Batch 50, Loss: 4.0264952851022994e-12\n",
      "Epoch 85, Batch 60, Loss: 5.204170427930425e-18\n",
      "Epoch 85, Batch 70, Loss: 6.161142780614936e-12\n",
      "Epoch 85, Batch 80, Loss: 1.2122742048197555e-12\n",
      "Epoch 86, Batch 0, Loss: 3.1762006220027107e-13\n",
      "Epoch 86, Batch 10, Loss: 5.715913853343973e-16\n",
      "Epoch 86, Batch 20, Loss: 9.064697787634537e-12\n",
      "Epoch 86, Batch 30, Loss: -0.0\n",
      "Epoch 86, Batch 40, Loss: 1.0847986455334673e-09\n",
      "Epoch 86, Batch 50, Loss: 5.741934705483654e-16\n",
      "Epoch 86, Batch 60, Loss: 4.510656328476663e-06\n",
      "Epoch 86, Batch 70, Loss: 0.005788138949424351\n",
      "Epoch 86, Batch 80, Loss: 1.6824051308288823e-08\n",
      "Epoch 87, Batch 0, Loss: 5.217306251874977e-09\n",
      "Epoch 87, Batch 10, Loss: 0.010926063408664572\n",
      "Epoch 87, Batch 20, Loss: 2.1868327490044554e-11\n",
      "Epoch 87, Batch 30, Loss: 0.018109301936908045\n",
      "Epoch 87, Batch 40, Loss: 0.00577789125283976\n",
      "Epoch 87, Batch 50, Loss: 4.400126096817633e-15\n",
      "Epoch 87, Batch 60, Loss: 7.047661065914551e-14\n",
      "Epoch 87, Batch 70, Loss: 0.0006169973186276334\n",
      "Epoch 87, Batch 80, Loss: 0.01476684865692299\n",
      "Epoch 88, Batch 0, Loss: 2.0462798122676015e-14\n",
      "Epoch 88, Batch 10, Loss: 2.2040222465714796e-06\n",
      "Epoch 88, Batch 20, Loss: 1.8275129319189518e-06\n",
      "Epoch 88, Batch 30, Loss: 0.00030020986189746877\n",
      "Epoch 88, Batch 40, Loss: 0.01667009491854637\n",
      "Epoch 88, Batch 50, Loss: 1.618145291250781e-11\n",
      "Epoch 88, Batch 60, Loss: 8.890958841828299e-07\n",
      "Epoch 88, Batch 70, Loss: 1.7572748811647635e-15\n",
      "Epoch 88, Batch 80, Loss: 6.336218013702763e-12\n",
      "Epoch 89, Batch 0, Loss: 0.05087497528771916\n",
      "Epoch 89, Batch 10, Loss: 1.5066246861367292e-13\n",
      "Epoch 89, Batch 20, Loss: -0.0\n",
      "Epoch 89, Batch 30, Loss: 1.631168453312455e-08\n",
      "Epoch 89, Batch 40, Loss: 0.02770149875398974\n",
      "Epoch 89, Batch 50, Loss: 9.976577826416665e-08\n",
      "Epoch 89, Batch 60, Loss: 8.666125118781756e-12\n",
      "Epoch 89, Batch 70, Loss: 1.7347234759768075e-18\n",
      "Epoch 89, Batch 80, Loss: 7.459310946700342e-17\n",
      "Epoch 90, Batch 0, Loss: 0.002930667397658619\n",
      "Epoch 90, Batch 10, Loss: 0.0007826668325177612\n",
      "Epoch 90, Batch 20, Loss: 0.06878888235188756\n",
      "Epoch 90, Batch 30, Loss: 7.80625564189564e-18\n",
      "Epoch 90, Batch 40, Loss: 4.344591878084307e-05\n",
      "Epoch 90, Batch 50, Loss: 4.254347746753583e-09\n",
      "Epoch 90, Batch 60, Loss: 6.798576595002913e-10\n",
      "Epoch 90, Batch 70, Loss: 2.2059725450158008e-07\n",
      "Epoch 90, Batch 80, Loss: 2.699730998505492e-05\n",
      "Epoch 91, Batch 0, Loss: 1.5186152186703407e-09\n",
      "Epoch 91, Batch 10, Loss: 1.601149768326919e-15\n",
      "Epoch 91, Batch 20, Loss: 3.460314592984867e-08\n",
      "Epoch 91, Batch 30, Loss: 0.019826557518046146\n",
      "Epoch 91, Batch 40, Loss: 1.6731407925798032e-15\n",
      "Epoch 91, Batch 50, Loss: 2.5153490401663783e-17\n",
      "Epoch 91, Batch 60, Loss: 3.027092465579593e-16\n",
      "Epoch 91, Batch 70, Loss: 6.634222990244358e-06\n",
      "Epoch 91, Batch 80, Loss: 0.043468771473038306\n",
      "Epoch 92, Batch 0, Loss: 6.047191329026463e-08\n",
      "Epoch 92, Batch 10, Loss: 5.340378953737987e-07\n",
      "Epoch 92, Batch 20, Loss: 0.0284311430650822\n",
      "Epoch 92, Batch 30, Loss: 0.0001160915537954105\n",
      "Epoch 92, Batch 40, Loss: 1.0559394919040663e-06\n",
      "Epoch 92, Batch 50, Loss: 3.594106947455108e-10\n",
      "Epoch 92, Batch 60, Loss: 0.0851125644650245\n",
      "Epoch 92, Batch 70, Loss: 2.985911475038793e-05\n",
      "Epoch 92, Batch 80, Loss: 0.0004835497413499109\n",
      "Epoch 93, Batch 0, Loss: 2.3418766925686967e-17\n",
      "Epoch 93, Batch 10, Loss: 2.5812685322620175e-14\n",
      "Epoch 93, Batch 20, Loss: 2.1435960146271092e-10\n",
      "Epoch 93, Batch 30, Loss: 3.595336364278359e-08\n",
      "Epoch 93, Batch 40, Loss: -0.0\n",
      "Epoch 93, Batch 50, Loss: 3.79367403030889e-11\n",
      "Epoch 93, Batch 60, Loss: 1.003631119876663e-11\n",
      "Epoch 93, Batch 70, Loss: 3.2873877231583295e-14\n",
      "Epoch 93, Batch 80, Loss: 1.8474805019153417e-16\n",
      "Epoch 94, Batch 0, Loss: 3.8728775321173925e-09\n",
      "Epoch 94, Batch 10, Loss: 2.6020852139652114e-18\n",
      "Epoch 94, Batch 20, Loss: 6.463336810558149e-13\n",
      "Epoch 94, Batch 30, Loss: 5.3516219234249533e-14\n",
      "Epoch 94, Batch 40, Loss: 6.555117051085774e-11\n",
      "Epoch 94, Batch 50, Loss: 5.901182320622477e-14\n",
      "Epoch 94, Batch 60, Loss: 1.1859437043533445e-14\n",
      "Epoch 94, Batch 70, Loss: 6.210310043997461e-16\n",
      "Epoch 94, Batch 80, Loss: 2.7016077628772453e-05\n",
      "Epoch 95, Batch 0, Loss: 9.58799012418706e-14\n",
      "Epoch 95, Batch 10, Loss: 2.6020852139652114e-18\n",
      "Epoch 95, Batch 20, Loss: 7.925502608966961e-11\n",
      "Epoch 95, Batch 30, Loss: 1.1232334506951441e-15\n",
      "Epoch 95, Batch 40, Loss: 2.0296264668929148e-16\n",
      "Epoch 95, Batch 50, Loss: 3.122502256758263e-17\n",
      "Epoch 95, Batch 60, Loss: 4.570996359199152e-16\n",
      "Epoch 95, Batch 70, Loss: -0.0\n",
      "Epoch 95, Batch 80, Loss: 2.1727411536669936e-14\n",
      "Epoch 96, Batch 0, Loss: 1.6279125649393746e-11\n",
      "Epoch 96, Batch 10, Loss: 8.786400847413089e-10\n",
      "Epoch 96, Batch 20, Loss: 1.436451279495413e-10\n",
      "Epoch 96, Batch 30, Loss: 2.0961133059058553e-11\n",
      "Epoch 96, Batch 40, Loss: 5.373325052244147e-12\n",
      "Epoch 96, Batch 50, Loss: 1.465841337200428e-16\n",
      "Epoch 96, Batch 60, Loss: 2.5796899343298596e-12\n",
      "Epoch 96, Batch 70, Loss: 9.035835248552371e-11\n",
      "Epoch 96, Batch 80, Loss: 1.4970663597682579e-15\n",
      "Epoch 97, Batch 0, Loss: 1.192362181214467e-14\n",
      "Epoch 97, Batch 10, Loss: 1.1152317504580295e-10\n",
      "Epoch 97, Batch 20, Loss: 7.124709530767357e-10\n",
      "Epoch 97, Batch 30, Loss: 3.1381881770265957e-10\n",
      "Epoch 97, Batch 40, Loss: 9.193328401039535e-12\n",
      "Epoch 97, Batch 50, Loss: 1.3941278688012844e-12\n",
      "Epoch 97, Batch 60, Loss: 7.052518291590075e-15\n",
      "Epoch 97, Batch 70, Loss: 1.5005358067202263e-15\n",
      "Epoch 97, Batch 80, Loss: 1.5196177649559767e-15\n",
      "Epoch 98, Batch 0, Loss: 7.888755874050422e-11\n",
      "Epoch 98, Batch 10, Loss: -0.0\n",
      "Epoch 98, Batch 20, Loss: 6.416631255919778e-10\n",
      "Epoch 98, Batch 30, Loss: -0.0\n",
      "Epoch 98, Batch 40, Loss: 6.847820921424439e-15\n",
      "Epoch 98, Batch 50, Loss: 3.469446951953616e-18\n",
      "Epoch 98, Batch 60, Loss: 4.486843853365449e-09\n",
      "Epoch 98, Batch 70, Loss: 8.827443433337384e-11\n",
      "Epoch 98, Batch 80, Loss: 1.5005358067199667e-16\n",
      "Epoch 99, Batch 0, Loss: 1.2793585635331047e-15\n",
      "Epoch 99, Batch 10, Loss: 1.628073027011244e-11\n",
      "Epoch 99, Batch 20, Loss: 3.469446951953616e-18\n",
      "Epoch 99, Batch 30, Loss: -0.0\n",
      "Epoch 99, Batch 40, Loss: 5.0030292409221096e-14\n",
      "Epoch 99, Batch 50, Loss: 2.103326193826287e-13\n",
      "Epoch 99, Batch 60, Loss: 1.4734823220488136e-07\n",
      "Epoch 99, Batch 70, Loss: 9.481998519700735e-15\n",
      "Epoch 99, Batch 80, Loss: 1.9119688471777377e-13\n",
      "Epoch 100, Batch 0, Loss: 1.3661120845872115e-13\n",
      "Epoch 100, Batch 10, Loss: 4.250072516143201e-17\n",
      "Epoch 100, Batch 20, Loss: 1.4406366726408049e-12\n",
      "Epoch 100, Batch 30, Loss: 1.2570214482402588e-07\n",
      "Epoch 100, Batch 40, Loss: 4.0241818057664235e-09\n",
      "Epoch 100, Batch 50, Loss: 1.5057486507942697e-13\n",
      "Epoch 100, Batch 60, Loss: 7.487691023433773e-13\n",
      "Epoch 100, Batch 70, Loss: -0.0\n",
      "Epoch 100, Batch 80, Loss: 9.801187639269075e-17\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.78%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test\n",
    "\n",
    "accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.846243e-165</td>\n",
       "      <td>7.028159e-142</td>\n",
       "      <td>2.419346e-188</td>\n",
       "      <td>1.889639e-62</td>\n",
       "      <td>2.120724e-100</td>\n",
       "      <td>2.888539e-71</td>\n",
       "      <td>3.462527e-122</td>\n",
       "      <td>1.517826e-124</td>\n",
       "      <td>6.896861e-80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.614139e-184</td>\n",
       "      <td>1.200067e-141</td>\n",
       "      <td>2.289627e-106</td>\n",
       "      <td>2.376350e-204</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.499014e-116</td>\n",
       "      <td>6.858467e-177</td>\n",
       "      <td>3.495019e-81</td>\n",
       "      <td>3.320043e-113</td>\n",
       "      <td>7.965949e-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.771995e-129</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.703077e-103</td>\n",
       "      <td>4.794512e-93</td>\n",
       "      <td>1.252814e-92</td>\n",
       "      <td>1.085145e-76</td>\n",
       "      <td>8.064488e-137</td>\n",
       "      <td>5.768229e-88</td>\n",
       "      <td>2.110432e-74</td>\n",
       "      <td>3.177723e-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.402105e-82</td>\n",
       "      <td>2.380122e-153</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.199895e-102</td>\n",
       "      <td>1.313282e-142</td>\n",
       "      <td>4.287808e-158</td>\n",
       "      <td>1.490857e-153</td>\n",
       "      <td>5.633997e-159</td>\n",
       "      <td>1.690953e-109</td>\n",
       "      <td>1.906012e-134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.488476e-126</td>\n",
       "      <td>6.921428e-81</td>\n",
       "      <td>2.675929e-99</td>\n",
       "      <td>3.854714e-77</td>\n",
       "      <td>9.852448e-57</td>\n",
       "      <td>2.824233e-58</td>\n",
       "      <td>2.182112e-94</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.530507e-144</td>\n",
       "      <td>3.250408e-47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1              2              3              4  \\\n",
       "0   1.000000e+00  4.846243e-165  7.028159e-142  2.419346e-188   1.889639e-62   \n",
       "1  6.614139e-184  1.200067e-141  2.289627e-106  2.376350e-204   1.000000e+00   \n",
       "2  1.771995e-129   1.000000e+00  4.703077e-103   4.794512e-93   1.252814e-92   \n",
       "3   7.402105e-82  2.380122e-153   1.000000e+00  6.199895e-102  1.313282e-142   \n",
       "4  3.488476e-126   6.921428e-81   2.675929e-99   3.854714e-77   9.852448e-57   \n",
       "\n",
       "               5              6              7              8              9  \n",
       "0  2.120724e-100   2.888539e-71  3.462527e-122  1.517826e-124   6.896861e-80  \n",
       "1  4.499014e-116  6.858467e-177   3.495019e-81  3.320043e-113   7.965949e-72  \n",
       "2   1.085145e-76  8.064488e-137   5.768229e-88   2.110432e-74   3.177723e-85  \n",
       "3  4.287808e-158  1.490857e-153  5.633997e-159  1.690953e-109  1.906012e-134  \n",
       "4   2.824233e-58   2.182112e-94   1.000000e+00  5.530507e-144   3.250408e-47  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9\n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_df = pd.DataFrame(y_true)\n",
    "y_true_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
